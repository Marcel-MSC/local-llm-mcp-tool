# Path to the GGUF model file
# Leave empty to use default model or download automatically
MODEL_PATH=

# URL to download model automatically (optional)
# Example: https://huggingface.co/...
MODEL_URL=

# Model configurations
CONTEXT_SIZE=2048
N_THREADS=4
N_GPU_LAYERS=0

# Session / history configuration
# Directory (relative to server.py) where conversation history will be stored
SESSION_HISTORY_DIR=history

# Maximum number of messages to keep per session (most recent are kept)
SESSION_MAX_MESSAGES=40

# Approximate maximum size in bytes per session file (older messages are trimmed)
SESSION_MAX_FILE_BYTES=2097152

# Whether to automatically trim history files when limits are exceeded (true/false)
SESSION_AUTO_TRIM=true

# Streaming configuration
# Enable streaming responses (tokens/chunks sent incrementally instead of waiting for full response)
STREAMING_ENABLED=false

# Approximate chunk size for streaming (in characters). Smaller values = more frequent updates but more overhead
STREAMING_CHUNK_SIZE=50
